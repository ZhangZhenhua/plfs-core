###############
# plfsrc rules
###############
# 
# Official rule:
# The backends directive is IMMUTABLE.  Once a mount point is defined, the
# backends can NEVER be changed.  If multiple machines (e.g. FTA's and multiple
# clusters) share a PLFS mount point, they must all define the same exact set of
# backends in the same exact ordering.
# 
# All other directives are merely hints and can be set to any value without
# affecting correctness. 
# 
# Explanation:
# Once a mount point is assigned a set of backends, the mapping between that
# mount point and that set of backends can NEVER be changed.  Changing backends
# for an existing mount point WILL ORPHAN files on that mount point.  If multiple
# clusters and FTA's share a PLFS mount point, they MUST have the same exact set
# of backends, in the same exact order, defined for that mount point.
# 
# NO OTHER values in the plfsrc affect correctness.  They are just hints to help
# with performance.  Therefore we can safely have different values for
# threadpool_size and num_hostdirs on different machines even if those machines
# share a mount point.  We can change values for threadpool_size and num_hostdirs
# whenever we want.  Existing files will not be adversely affected in any way
# although performance may change.


#################
# num_hostdirs:
# this should be square root of total number of compute nodes
#################
# 
# Official rule:
# Let N be the size in PE's of the largest job that will be run on the system.
# Let S be the sqrt of that.
# Let P be the lowest prime number greater than or equal to S.
# Set num_hostdirs to be P.
# 
# For example, say that the largest job that can be run on cluster C is 10K PEs.
# num_hostdirs should be set to 101.
# 
# Explanation:
# 
# PLFS stores file data and file metadata in a PLFS structure called a container
# which is built using directories and files on an underlying file system.  The
# number of files within a container is basically equal to the number of PE's
# that wrote the file.  We store these files within subdirs within the container.
# At exascale, we might have a billion cores and we cannot put a billion files in
# a single directory.  Therefore we use num_hostdirs to create a balanced internal
# hierarchy within the container.  If there are 10K PE's, this means that we will
# have 100 hostdirs and each of those hostdirs will have 100 files in them.  However,
# the distribution of files into subdirs is not deterministic; rather we rely on
# hashing.  Hashing plays well with prime numbers; that's why we do the prime number
# bit.
# 
# [This means that for 1B cores, we will have about 30K files per directory.  We
# might need to move to a 3 level hierarchy for exascale.]

num_hostdirs 79


###############
# threadpool_size:
# this should be related to how many threads you want running on a machine
###############
#
# Official rule:
# If compute node, 1.  If FTA, 16.
# 
# Explanation:
# We observed that for two different read operations, the FTA performance was
# very low due to a lack of concurrency resulting in too few PanFS components
# being engaged.  One operation was reading all the multiple index files within
# the container which PLFS does on the read open().  The second was reading from
# multiple data files within the container to fill in a large logical read.  We
# then added threads to increase concurrency, engage more PanFS hardware, and got
# much better results.  But we assume that large jobs run on the compute nodes so
# we don't want to add even more concurrency since the large number of PE's
# should provide sufficient concurrency to engage all of the PanFS hardware; in
# fact, adding more streams can hurt per client PanFS performance.  This is just
# a performance hint; it can be set to any value without affecting correctness.

threadpool_size 1

# this must match where FUSE is mounted and the logical paths passed to ADIO
mount_point /plfs/mile

# these must be full paths, can be just a single one
backends /panfs/mile/vol1/.plfs_store,/panfs/mile/vol2/.plfs_store,/panfs/mile/vol3/.plfs_store,/panfs/mile/vol4/.plfs_store


# this must match where FUSE is mounted and the logical paths passed to ADIO
mount_point /plfs/scratch

# these must be full paths, can be just a single one
backends /panfs/scratch/vol1/.plfs_store,/panfs/scratch/vol2/.plfs_store,/panfs/scratch/vol3/.plfs_store,/panfs/scratch/vol4/.plfs_store,/panfs/scratch/vol5/.plfs_store,/panfs/scratch/vol6/.plfs_store,/panfs/scratch/vol7/.plfs_store,/panfs/scratch/vol8/.plfs_store,/panfs/scratch/vol9/.plfs_store,/panfs/scratch/vol10/.plfs_store,/panfs/scratch/vol11/.plfs_store,/panfs/scratch/vol12/.plfs_store,/panfs/scratch/vol13/.plfs_store,/panfs/scratch/vol14/.plfs_store,/panfs/scratch/vol15/.plfs_store,/panfs/scratch/vol16/.plfs_store,/panfs/scratch/vol17/.plfs_store,/panfs/scratch/vol18/.plfs_store,/panfs/scratch/vol19/.plfs_store,/panfs/scratch/vol20/.plfs_store,/panfs/scratch/vol21/.plfs_store,/panfs/scratch/vol22/.plfs_store,/panfs/scratch/vol23/.plfs_store,/panfs/scratch/vol24/.plfs_store,/panfs/scratch/vol25/.plfs_store,/panfs/scratch/vol26/.plfs_store,/panfs/scratch/vol27/.plfs_store,/panfs/scratch/vol28/.plfs_store,/panfs/scratch/vol29/.plfs_store,/panfs/scratch/vol30/.plfs_store,/panfs/scratch/vol31/.plfs_store,/panfs/scratch/vol32/.plfs_store,/panfs/scratch/vol33/.plfs_store,/panfs/scratch/vol34/.plfs_store,/panfs/scratch/vol35/.plfs_store,/panfs/scratch/vol36/.plfs_store,/panfs/scratch/vol37/.plfs_store,/panfs/scratch/vol38/.plfs_store,/panfs/scratch/vol39/.plfs_store

# this must match where FUSE is mounted and the logical paths passed to ADIO
mount_point /plfs/scratch2

# these must be full paths, can be just a single one
backends /panfs/scratch2/vol1/.plfs_store,/panfs/scratch2/vol2/.plfs_store,/panfs/scratch2/vol3/.plfs_store,/panfs/scratch2/vol4/.plfs_store,/panfs/scratch2/vol5/.plfs_store,/panfs/scratch2/vol6/.plfs_store,/panfs/scratch2/vol7/.plfs_store,/panfs/scratch2/vol8/.plfs_store,/panfs/scratch2/vol9/.plfs_store,/panfs/scratch2/vol10/.plfs_store,/panfs/scratch2/vol11/.plfs_store,/panfs/scratch2/vol12/.plfs_store

# this must match where FUSE is mounted and the logical paths passed to ADIO
mount_point /plfs/scratch3

# these must be full paths, can be just a single one
backends /panfs/scratch3/vol14/.plfs_store,/panfs/scratch3/vol15/.plfs_store,/panfs/scratch3/vol16/.plfs_store,/panfs/scratch3/vol17/.plfs_store,/panfs/scratch3/vol18/.plfs_store,/panfs/scratch3/vol19/.plfs_store,/panfs/scratch3/vol20/.plfs_store,/panfs/scratch3/vol21/.plfs_store,/panfs/scratch3/vol22/.plfs_store,/panfs/scratch3/vol23/.plfs_store,/panfs/scratch3/vol24/.plfs_store,/panfs/scratch3/vol25/.plfs_store,/panfs/scratch3/vol26/.plfs_store,/panfs/scratch3/vol27/.plfs_store

# this must match where FUSE is mounted and the logical paths passed to ADIO
mount_point /plfs/scratch6

# these must be full paths, can be just a single one
backends /panfs/scratch6/vol11/.plfs_store,/panfs/scratch6/vol12/.plfs_store,/panfs/scratch6/vol13/.plfs_store,/panfs/scratch6/vol14/.plfs_store,/panfs/scratch6/vol15/.plfs_store,/panfs/scratch6/vol16/.plfs_store,/panfs/scratch6/vol17/.plfs_store
